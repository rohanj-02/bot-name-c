{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-27T09:11:17.804594Z","iopub.execute_input":"2021-11-27T09:11:17.804894Z","iopub.status.idle":"2021-11-27T09:11:17.809915Z","shell.execute_reply.started":"2021-11-27T09:11:17.804866Z","shell.execute_reply":"2021-11-27T09:11:17.808316Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Attention Class","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras import backend as K\n\n\nclass AttentionLayer(Layer):\n    \"\"\"\n    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n    There are three sets of weights introduced W_a, U_a, and V_a\n     \"\"\"\n\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        # Create a trainable weight variable for this layer.\n\n        self.W_a = self.add_weight(name='W_a',\n                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.U_a = self.add_weight(name='U_a',\n                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.V_a = self.add_weight(name='V_a',\n                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer='uniform',\n                                   trainable=True)\n\n        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, inputs, verbose=False):\n        \"\"\"\n        inputs: [encoder_output_sequence, decoder_output_sequence]\n        \"\"\"\n        assert type(inputs) == list\n        encoder_out_seq, decoder_out_seq = inputs\n        if verbose:\n            print('encoder_out_seq>', encoder_out_seq.shape)\n            print('decoder_out_seq>', decoder_out_seq.shape)\n\n        def energy_step(inputs, states):\n            \"\"\" Step function for computing energy for a single decoder state\n            inputs: (batchsize * 1 * de_in_dim)\n            states: (batchsize * 1 * de_latent_dim)\n            \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            \"\"\" Some parameters required for shaping tensors\"\"\"\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n            # <= batch size * en_seq_len * latent_dim\n            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n\n            \"\"\" Computing hj.Ua \"\"\"\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n            if verbose:\n                print('Ua.h>', U_a_dot_h.shape)\n\n            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n            # <= batch_size*en_seq_len, latent_dim\n            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n            if verbose:\n                print('Ws+Uh>', Ws_plus_Uh.shape)\n\n            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n            # <= batch_size, en_seq_len\n            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n            # <= batch_size, en_seq_len\n            e_i = K.softmax(e_i)\n\n            if verbose:\n                print('ei>', e_i.shape)\n\n            return e_i, [e_i]\n\n        def context_step(inputs, states):\n            \"\"\" Step function for computing ci using ei \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            # <= batch_size, hidden_size\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            if verbose:\n                print('ci>', c_i.shape)\n            return c_i, [c_i]\n\n        fake_state_c = K.sum(encoder_out_seq, axis=1)\n        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n\n        \"\"\" Computing energy outputs \"\"\"\n        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n        last_out, e_outputs, _ = K.rnn(\n            energy_step, decoder_out_seq, [fake_state_e],\n        )\n\n        \"\"\" Computing context vectors \"\"\"\n        last_out, c_outputs, _ = K.rnn(\n            context_step, e_outputs, [fake_state_c],\n        )\n\n        return c_outputs, e_outputs\n\n    def compute_output_shape(self, input_shape):\n        \"\"\" Outputs produced by the layer \"\"\"\n        return [\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n        ]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:17.882982Z","iopub.execute_input":"2021-11-27T09:11:17.883251Z","iopub.status.idle":"2021-11-27T09:11:17.910322Z","shell.execute_reply.started":"2021-11-27T09:11:17.883221Z","shell.execute_reply":"2021-11-27T09:11:17.909303Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import re\n\n# lines = open('../input/chatbot-data/cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8',\n#              errors='ignore').read().split('\\n')\n\n# convers = open('../input/chatbot-data/cornell movie-dialogs corpus/movie_conversations.txt', encoding='utf-8',\n#              errors='ignore').read().split('\\n')\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-11-27T09:11:17.915186Z","iopub.execute_input":"2021-11-27T09:11:17.915524Z","iopub.status.idle":"2021-11-27T09:11:17.922424Z","shell.execute_reply.started":"2021-11-27T09:11:17.915486Z","shell.execute_reply":"2021-11-27T09:11:17.921622Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# len(lines)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:17.937796Z","iopub.execute_input":"2021-11-27T09:11:17.938042Z","iopub.status.idle":"2021-11-27T09:11:17.941488Z","shell.execute_reply.started":"2021-11-27T09:11:17.938016Z","shell.execute_reply":"2021-11-27T09:11:17.940638Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocess","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nimport nltk\n\ntarget_counter = Counter()\ninput_counter = Counter()\nMAX_TARGET_SEQ_LENGTH = 40\ninput_texts = []\ntarget_texts = []\nalphas = 'abcdefghijklmnopqrstuvwxyz1234567890'\ndef permissible_chars(word):\n\n    for char in word:\n        if char in alphas:\n            return True\n\n    return False\n# Parser base code of GuntherCox dataset obtained from link below and modified as per requirement.\n# https://github.com/kushagra2101/ChatCrazie/blob/master/train_seq2seq.py\n\nDATA_DIR_PATH = '../input/response-generation/data'\nfor file in os.listdir(DATA_DIR_PATH):\n    filepath = os.path.join(DATA_DIR_PATH, file)\n    if os.path.isfile(filepath):\n        print('processing file: ', file)\n        lines = open(filepath, 'rt', encoding='utf8').read().split('\\n')\n        prev_words = []\n        for line in lines:\n\n            if line.startswith('- - '):\n                prev_words = []\n\n            if line.startswith('- - ') or line.startswith('  - '):\n                line = line.replace('- - ', '')\n                line = line.replace('  - ', '')\n                next_words = [w.lower() for w in nltk.word_tokenize(line)]\n                next_words = [w for w in next_words if permissible_chars(w)]\n                if len(next_words) > MAX_TARGET_SEQ_LENGTH:\n                    next_words = next_words[0:MAX_TARGET_SEQ_LENGTH]\n\n                if len(prev_words) > 0:\n                    input_texts.append(prev_words)\n                    for w in prev_words:\n                        input_counter[w] += 1\n\n                    target_words = next_words[:]\n                    for w in target_words:\n                        target_counter[w] += 1\n                    target_texts.append(target_words)\n\n                prev_words = next_words\n\n\n\n# for idx, (input_words, target_words) in enumerate(zip(input_texts, target_texts)):\n#     if idx < 20:\n#         print([input_words, target_words])\n# for i in input_texts:\n    # input_texts[i] = ' '.join(input_texts[i])\n\n# input_w2i, input_i2w, target_w2i, target_i2w = {},{},{},{}\nquestions = []\nanswers = []\nfor sentence in input_texts:\n    questions.append(' '.join(sentence))\n\nfor sentence in target_texts:\n    answers.append(' '.join(sentence))\n# print(input_texts[:5])\n# print(target_texts[:5])\nprint(questions[:5])\nprint(answers[:5])","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:18.060960Z","iopub.execute_input":"2021-11-27T09:11:18.061282Z","iopub.status.idle":"2021-11-27T09:11:27.894903Z","shell.execute_reply.started":"2021-11-27T09:11:18.061250Z","shell.execute_reply":"2021-11-27T09:11:27.893937Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# exchn = []\n# for conver in convers:\n#     exchn.append(conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\",\"\").split())\n\n# diag = {}\n# for line in lines:\n#     diag[line.split(' +++$+++ ')[0]] = line.split(' +++$+++ ')[-1]\n\n\n\n# ## delete\n# del(lines, convers,conver, line)\n\n\n\n# questions = []\n# answers = []\n\n# for conver in exchn:\n#     for i in range(len(conver) - 1):\n#         questions.append(diag[conver[i]])\n#         answers.append(diag[conver[i+1]])\n        \n        \n\n\n# ## delete\n# del(diag, exchn, conver, i)\n\n\n###############################\n#        max_len = 13         #\n###############################\n\nsorted_ques = []\nsorted_ans = []\nfor i in range(len(questions)):\n        sorted_ques.append(questions[i])\n        sorted_ans.append(answers[i])\n\n\n\n###############################\n#                             #\n###############################\n\n\n\n\ndef clean_text(txt):\n    txt = txt.lower()\n    txt = re.sub(r\"i'm\", \"i am\", txt)\n    txt = re.sub(r\"he's\", \"he is\", txt)\n    txt = re.sub(r\"she's\", \"she is\", txt)\n    txt = re.sub(r\"that's\", \"that is\", txt)\n    txt = re.sub(r\"what's\", \"what is\", txt)\n    txt = re.sub(r\"where's\", \"where is\", txt)\n    txt = re.sub(r\"\\'ll\", \" will\", txt)\n    txt = re.sub(r\"\\'ve\", \" have\", txt)\n    txt = re.sub(r\"\\'re\", \" are\", txt)\n    txt = re.sub(r\"\\'d\", \" would\", txt)\n    txt = re.sub(r\"won't\", \"will not\", txt)\n    txt = re.sub(r\"can't\", \"can not\", txt)\n    txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n    return txt\n\nclean_ques = []\nclean_ans = []\n\nfor line in sorted_ques:\n    clean_ques.append(clean_text(line))\n        \nfor line in sorted_ans:\n    clean_ans.append(clean_text(line))\n\n\n\n## delete\ndel(answers, questions, line)\n\n\n###############################\n#                             #\n###############################\n\n\nfor i in range(len(clean_ans)):\n    clean_ans[i] = ' '.join(clean_ans[i].split()[:11])\n\n\n\n###############################\n#                             #\n###############################\n\ndel(sorted_ans, sorted_ques)\n\n\n## trimming\n# clean_ans=clean_ans[:30000]\n# clean_ques=clean_ques[:30000]\n## delete\n\n\n###  count occurences ###\nword2count = {}\n\nfor line in clean_ques:\n    for word in line.split():\n        if word not in word2count:\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\nfor line in clean_ans:\n    for word in line.split():\n        if word not in word2count:\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\n\n## delete\ndel(word, line)\n\n\n###  remove less frequent ###\nthresh = 2\n\nvocab = {}\nword_num = 0\nfor word, count in word2count.items():\n    if count >= thresh:\n        vocab[word] = word_num\n        word_num += 1\n        \n## delete\ndel(word2count, word, count, thresh)       \ndel(word_num)        \n\n\n\nfor i in range(len(clean_ans)):\n    clean_ans[i] = '<SOS> ' + clean_ans[i] + ' <EOS>'\n\n\n\ntokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\nx = len(vocab)\nfor token in tokens:\n    vocab[token] = x\n    x += 1\n    \n\n# vocab['cameron'] = vocab['<PAD>']\n# vocab['<PAD>'] = 0\n\n## delete\ndel(token, tokens) \ndel(x)\n\n### inv answers dict ###\ninv_vocab = {w:v for v, w in vocab.items()}\n\n\n\n## delete\ndel(i)\n\n\nencoder_inp = []\ncounter = 0\nfor line in clean_ques:\n    lst = []\n    for word in line.split():\n        if word not in vocab:\n            counter += 1\n#             print(word)\n            lst.append(vocab['<OUT>'])\n        else:\n            lst.append(vocab[word])\n        \n    encoder_inp.append(lst)\n\ndecoder_inp = []\nfor line in clean_ans:\n    lst = []\n    for word in line.split():\n        if word not in vocab:\n            counter += 1\n#             print(word)\n            lst.append(vocab['<OUT>'])\n        else:\n            lst.append(vocab[word])        \n    decoder_inp.append(lst)\n\n### delete\nprint(counter)\n# del(clean_ans, clean_ques, line, lst, word)\n\n\nMAX_LEN = 15\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nencoder_inp = pad_sequences(encoder_inp, MAX_LEN, padding='post', truncating='post')\ndecoder_inp = pad_sequences(decoder_inp, MAX_LEN, padding='post', truncating='post')\n\n\n\n\ndecoder_final_output = []\nfor i in decoder_inp:\n    decoder_final_output.append(i[1:]) \n\ndecoder_final_output = pad_sequences(decoder_final_output, MAX_LEN, padding='post', truncating='post')\n\ndel(i)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:27.896856Z","iopub.execute_input":"2021-11-27T09:11:27.897148Z","iopub.status.idle":"2021-11-27T09:11:29.509071Z","shell.execute_reply.started":"2021-11-27T09:11:27.897122Z","shell.execute_reply":"2021-11-27T09:11:29.508298Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# decoder_final_output, decoder_final_input, encoder_final, vocab, inv_vocab\n\nVOCAB_SIZE = len(vocab)\n\nprint(decoder_final_output.shape, decoder_inp.shape, encoder_inp.shape, len(vocab), len(inv_vocab), inv_vocab[0])","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:29.511598Z","iopub.execute_input":"2021-11-27T09:11:29.511950Z","iopub.status.idle":"2021-11-27T09:11:29.518990Z","shell.execute_reply.started":"2021-11-27T09:11:29.511913Z","shell.execute_reply":"2021-11-27T09:11:29.517990Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pickle \nwith open(\"vocab_size.pkl\", \"wb\") as f:\n    pickle.dump(VOCAB_SIZE, f)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:39:07.013004Z","iopub.execute_input":"2021-11-27T09:39:07.013462Z","iopub.status.idle":"2021-11-27T09:39:07.018843Z","shell.execute_reply.started":"2021-11-27T09:39:07.013425Z","shell.execute_reply":"2021-11-27T09:39:07.018001Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"inv_vocab[16]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:29.520192Z","iopub.execute_input":"2021-11-27T09:11:29.520553Z","iopub.status.idle":"2021-11-27T09:11:29.530169Z","shell.execute_reply.started":"2021-11-27T09:11:29.520515Z","shell.execute_reply":"2021-11-27T09:11:29.529300Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#print(len(decoder_final_input), MAX_LEN, VOCAB_SIZE)\n#decoder_final_input[0]\n#decoder_output_data = np.zeros((len(decoder_final_input), MAX_LEN, VOCAB_SIZE), dtype=\"float32\")\n#print(decoder_output_data.shape)\n#decoder_final_input[80]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:29.534295Z","iopub.execute_input":"2021-11-27T09:11:29.534672Z","iopub.status.idle":"2021-11-27T09:11:29.538391Z","shell.execute_reply.started":"2021-11-27T09:11:29.534634Z","shell.execute_reply":"2021-11-27T09:11:29.537397Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\ndecoder_final_output = to_categorical(decoder_final_output, len(vocab))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:29.542138Z","iopub.execute_input":"2021-11-27T09:11:29.542517Z","iopub.status.idle":"2021-11-27T09:11:30.225081Z","shell.execute_reply.started":"2021-11-27T09:11:29.542481Z","shell.execute_reply":"2021-11-27T09:11:30.223988Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"decoder_final_output.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:30.228507Z","iopub.execute_input":"2021-11-27T09:11:30.228769Z","iopub.status.idle":"2021-11-27T09:11:30.236025Z","shell.execute_reply.started":"2021-11-27T09:11:30.228742Z","shell.execute_reply":"2021-11-27T09:11:30.235048Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Glove Embedding","metadata":{}},{"cell_type":"code","source":"\nembeddings_index = {}\nwith open('../input/glove6b/glove.6B.100d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\nprint(\"Glove Loded!\")\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:30.237690Z","iopub.execute_input":"2021-11-27T09:11:30.238083Z","iopub.status.idle":"2021-11-27T09:11:48.665730Z","shell.execute_reply.started":"2021-11-27T09:11:30.238043Z","shell.execute_reply":"2021-11-27T09:11:48.664855Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\nembedding_dimention = 100\ndef embedding_matrix_creater(embedding_dimention, word_index):\n    embedding_matrix = np.zeros((len(word_index)+1, embedding_dimention))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n          # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\nembedding_matrix = embedding_matrix_creater(100, word_index=vocab) \n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:48.668666Z","iopub.execute_input":"2021-11-27T09:11:48.668957Z","iopub.status.idle":"2021-11-27T09:11:48.687115Z","shell.execute_reply.started":"2021-11-27T09:11:48.668930Z","shell.execute_reply":"2021-11-27T09:11:48.686414Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"del(embeddings_index)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:48.690040Z","iopub.execute_input":"2021-11-27T09:11:48.690299Z","iopub.status.idle":"2021-11-27T09:11:48.811450Z","shell.execute_reply.started":"2021-11-27T09:11:48.690274Z","shell.execute_reply":"2021-11-27T09:11:48.810332Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"embedding_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:48.834679Z","iopub.execute_input":"2021-11-27T09:11:48.834950Z","iopub.status.idle":"2021-11-27T09:11:48.841529Z","shell.execute_reply.started":"2021-11-27T09:11:48.834922Z","shell.execute_reply":"2021-11-27T09:11:48.840616Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"embedding_matrix[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:48.842904Z","iopub.execute_input":"2021-11-27T09:11:48.843587Z","iopub.status.idle":"2021-11-27T09:11:48.852061Z","shell.execute_reply.started":"2021-11-27T09:11:48.843546Z","shell.execute_reply":"2021-11-27T09:11:48.851104Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional, Concatenate, Dropout, Attention\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:48.853684Z","iopub.execute_input":"2021-11-27T09:11:48.854160Z","iopub.status.idle":"2021-11-27T09:11:48.860612Z","shell.execute_reply.started":"2021-11-27T09:11:48.854116Z","shell.execute_reply":"2021-11-27T09:11:48.859385Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"embed = Embedding(VOCAB_SIZE+1, \n                  100, \n                  \n                  input_length=MAX_LEN,\n                  trainable=True)\n\nembed.build((None,))\nembed.set_weights([embedding_matrix])\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:48.861992Z","iopub.execute_input":"2021-11-27T09:11:48.862286Z","iopub.status.idle":"2021-11-27T09:11:50.715434Z","shell.execute_reply.started":"2021-11-27T09:11:48.862246Z","shell.execute_reply":"2021-11-27T09:11:50.714560Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"enc_inp = Input(shape=(MAX_LEN, ))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:50.716859Z","iopub.execute_input":"2021-11-27T09:11:50.717257Z","iopub.status.idle":"2021-11-27T09:11:50.725430Z","shell.execute_reply.started":"2021-11-27T09:11:50.717215Z","shell.execute_reply":"2021-11-27T09:11:50.724389Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#embed = Embedding(VOCAB_SIZE+1, 50, mask_zero=True, input_length=13)(enc_inp)\nenc_embed = embed(enc_inp)\nenc_lstm = Bidirectional(LSTM(400, return_state=True, dropout=0.05, return_sequences = True))\n\nencoder_outputs, forward_h, forward_c, backward_h, backward_c = enc_lstm(enc_embed)\n\nstate_h = Concatenate()([forward_h, backward_h])\nstate_c = Concatenate()([forward_c, backward_c])\n\nenc_states = [state_h, state_c]\n\n\ndec_inp = Input(shape=(MAX_LEN, ))\ndec_embed = embed(dec_inp)\ndec_lstm = LSTM(400*2, return_state=True, return_sequences=True, dropout=0.05)\noutput, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n\n# attention\nattn_layer = AttentionLayer()\n# attn_op = Attention()([encoder_outputs, output])\nattn_op, attn_state = attn_layer([encoder_outputs, output])\ndecoder_concat_input = Concatenate(axis=-1)([output, attn_op])\n\n# print(attn_op.shape)\ndec_dense = Dense(VOCAB_SIZE, activation='softmax')\nfinal_output = dec_dense(decoder_concat_input)\n\nmodel = Model([enc_inp, dec_inp], final_output)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:50.727123Z","iopub.execute_input":"2021-11-27T09:11:50.727871Z","iopub.status.idle":"2021-11-27T09:11:53.174154Z","shell.execute_reply.started":"2021-11-27T09:11:50.727825Z","shell.execute_reply":"2021-11-27T09:11:53.173316Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import keras\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:53.175526Z","iopub.execute_input":"2021-11-27T09:11:53.175882Z","iopub.status.idle":"2021-11-27T09:11:53.236794Z","shell.execute_reply.started":"2021-11-27T09:11:53.175844Z","shell.execute_reply":"2021-11-27T09:11:53.235735Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:53.238103Z","iopub.execute_input":"2021-11-27T09:11:53.238584Z","iopub.status.idle":"2021-11-27T09:11:53.256925Z","shell.execute_reply.started":"2021-11-27T09:11:53.238530Z","shell.execute_reply":"2021-11-27T09:11:53.256254Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model.fit([encoder_inp, decoder_inp], decoder_final_output, epochs=15, batch_size=24, validation_split=0.15)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:11:53.259069Z","iopub.execute_input":"2021-11-27T09:11:53.259612Z","iopub.status.idle":"2021-11-27T09:25:39.118419Z","shell.execute_reply.started":"2021-11-27T09:11:53.259571Z","shell.execute_reply":"2021-11-27T09:25:39.117582Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# inferece","metadata":{}},{"cell_type":"code","source":"model.save('chatbot.h5')\nmodel.save_weights('chatbot_weights.h5')","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:25:39.119840Z","iopub.execute_input":"2021-11-27T09:25:39.120175Z","iopub.status.idle":"2021-11-27T09:25:39.679385Z","shell.execute_reply.started":"2021-11-27T09:25:39.120138Z","shell.execute_reply":"2021-11-27T09:25:39.678600Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Attention Inference\n","metadata":{}},{"cell_type":"code","source":"enc_model = tf.keras.models.Model(enc_inp, [encoder_outputs, enc_states])\n\n\ndecoder_state_input_h = tf.keras.layers.Input(shape=( 400 * 2,))\ndecoder_state_input_c = tf.keras.layers.Input(shape=( 400 * 2,))\n\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n\ndecoder_outputs, state_h, state_c = dec_lstm(dec_embed , initial_state=decoder_states_inputs)\n\n\ndecoder_states = [state_h, state_c]\n\n#decoder_output = dec_dense(decoder_outputs)\n\ndec_model = tf.keras.models.Model([dec_inp, decoder_states_inputs],\n                                      [decoder_outputs] + decoder_states)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:25:39.682685Z","iopub.execute_input":"2021-11-27T09:25:39.682942Z","iopub.status.idle":"2021-11-27T09:25:39.921410Z","shell.execute_reply.started":"2021-11-27T09:25:39.682916Z","shell.execute_reply":"2021-11-27T09:25:39.920546Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"enc_model.save(\"encoder_model.h5\")\ndec_model.save(\"decoder_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:25:39.923161Z","iopub.execute_input":"2021-11-27T09:25:39.923760Z","iopub.status.idle":"2021-11-27T09:25:39.976071Z","shell.execute_reply.started":"2021-11-27T09:25:39.923718Z","shell.execute_reply":"2021-11-27T09:25:39.975091Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open(\"vocab.pkl\", \"wb\") as f:\n    pickle.dump(vocab, f)\nwith open(\"inv_vocab.pkl\", \"wb\") as f:\n    pickle.dump(inv_vocab, f)\n# with open(\"attn_layer.pkl\", \"wb\") as f:\n#     pickle.dump(attn_layer, f)\nwith open(\"dec_dense.pkl\", \"wb\") as f:\n    pickle.dump(dec_dense, f)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:34:46.535793Z","iopub.execute_input":"2021-11-27T09:34:46.536128Z","iopub.status.idle":"2021-11-27T09:34:46.645454Z","shell.execute_reply.started":"2021-11-27T09:34:46.536096Z","shell.execute_reply":"2021-11-27T09:34:46.644324Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"\nfrom keras.preprocessing.sequence import pad_sequences\nprint(\"##########################################\")\nprint(\"#       start chatting ver. 1.0          #\")\nprint(\"##########################################\")\n\n\nprepro1 = \"\"\nwhile prepro1 != 'q':\n    \n    prepro1 = input(\"you : \")\n    try:\n        prepro1 = clean_text(prepro1)\n        prepro = [prepro1]\n        \n        txt = []\n        for x in prepro:\n            lst = []\n            for y in x.split():\n                try:\n                    lst.append(vocab[y])\n                except:\n                    lst.append(vocab['<OUT>'])\n            txt.append(lst)\n        txt = pad_sequences(txt, 20, padding='post')\n\n\n        ###\n        enc_op, stat = enc_model.predict( txt )\n#         print(enc_op, stat)\n        empty_target_seq = np.zeros( ( 1 , 1) )\n        empty_target_seq[0, 0] = vocab['<SOS>']\n        stop_condition = False\n        decoded_translation = ''\n\n\n        while not stop_condition :\n\n            dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + stat )\n\n            ###\n            ###########################\n            attn_op, attn_state = attn_layer([enc_op, dec_outputs])\n            decoder_concat_input = Concatenate(axis=-1)([dec_outputs, attn_op])\n            decoder_concat_input = dec_dense(decoder_concat_input)\n            ###########################\n#             print(decoder_concat_input)\n            sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n\n            sampled_word = inv_vocab[sampled_word_index] + ' '\n#             print(sampled_word)\n            if sampled_word != '<EOS> ':\n                decoded_translation += sampled_word           \n\n\n            if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 20:\n                stop_condition = True\n\n            empty_target_seq = np.zeros( ( 1 , 1 ) )  \n            empty_target_seq[ 0 , 0 ] = sampled_word_index\n            stat = [ h , c ] \n\n        print(\"chatbot attention : \", decoded_translation )\n        print(\"==============================================\")\n\n    except:\n        print(\"sorry didn't got you , please type again :( \")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:25:39.977427Z","iopub.execute_input":"2021-11-27T09:25:39.977948Z","iopub.status.idle":"2021-11-27T09:29:17.849292Z","shell.execute_reply.started":"2021-11-27T09:25:39.977909Z","shell.execute_reply":"2021-11-27T09:29:17.845878Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"len(encoder_inp)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:29:17.850285Z","iopub.status.idle":"2021-11-27T09:29:17.850692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(decoder_inp)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:29:17.851932Z","iopub.status.idle":"2021-11-27T09:29:17.852497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_inp[1]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:29:17.854009Z","iopub.status.idle":"2021-11-27T09:29:17.854833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_encoder = encoder_inp[:1000]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:29:17.856077Z","iopub.status.idle":"2021-11-27T09:29:17.856911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_encoder)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:29:17.858150Z","iopub.status.idle":"2021-11-27T09:29:17.858977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_output = decoder_final_output[:1000]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:29:17.860233Z","iopub.status.idle":"2021-11-27T09:29:17.861053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_output[1:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:29:17.862304Z","iopub.status.idle":"2021-11-27T09:29:17.863108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_encoder[1:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:29:17.864362Z","iopub.status.idle":"2021-11-27T09:29:17.865150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input = clean_ans[:1000]\ntest_output = clean_ques[:1000]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:29:17.866394Z","iopub.status.idle":"2021-11-27T09:29:17.867192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom tqdm import tqdm\n\nnum_examples = 500\n\nchencherry = SmoothingFunction()\ntotal_bleu = 0\nfor idx in tqdm(range(num_examples)):\n    prepro1 = test_input[idx]\n    prepro1 = clean_text(prepro1)\n    prepro = [prepro1]\n\n\n    txt = []\n    for x in prepro:\n        lst = []\n        for y in x.split():\n            try:\n                lst.append(vocab[y])\n            except:\n                lst.append(vocab['<OUT>'])\n        txt.append(lst)\n    txt = pad_sequences(txt, 20, padding='post')\n\n    enc_op, stat = enc_model.predict( txt )\n    #         print(enc_op, stat)\n    empty_target_seq = np.zeros( ( 1 , 1) )\n    empty_target_seq[0, 0] = vocab['<SOS>']\n    stop_condition = False\n    decoded_translation = ''\n    while not stop_condition :\n\n        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + stat )\n\n        ###\n        ###########################\n        attn_op, attn_state = attn_layer([enc_op, dec_outputs])\n        decoder_concat_input = Concatenate(axis=-1)([dec_outputs, attn_op])\n        decoder_concat_input = dec_dense(decoder_concat_input)\n        ###########################\n    #             print(decoder_concat_input)\n        sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n\n        sampled_word = inv_vocab[sampled_word_index] + ' '\n    #             print(sampled_word)\n        if sampled_word != '<EOS> ':\n            decoded_translation += sampled_word           \n\n\n        if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 20:\n            stop_condition = True\n\n        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n        empty_target_seq[ 0 , 0 ] = sampled_word_index\n        stat = [ h , c ] \n    total_bleu += sentence_bleu([decoded_translation.split()], test_output[idx].split(), smoothing_function = chencherry.method1)\n#     print(\"BLEU Score\", sentence_bleu([decoded_translation.split()], test_output[idx].split(), smoothing_function = chencherry.method1))\n    \nprint(\"Average BLEU:\", total_bleu / num_examples )","metadata":{"execution":{"iopub.status.busy":"2021-11-27T09:29:17.868488Z","iopub.status.idle":"2021-11-27T09:29:17.869293Z"},"trusted":true},"execution_count":null,"outputs":[]}]}